ID,Question,Section,Example,Information,Resources
1,What is the primary purpose of your AI system?,System Information,e.g. Chatbot for customer support ,,
2,Is the AI system intended to be used as a safety component of a product or is the AI system itself a product covered by the Union harmonisation legislation listed in Annex I AIA?,System Information,e.g. Yes it is a safety component in an autonomous vehicle.,,
3,Is the AI system itself a product being required to undergo a third-party conformity assessment with a view to the placing on the market or the putting into service of that product?,System Information,e.g. Yes it requires certification before deployment,,
4,Does the area (domain) of your AI system apply to critical infrastructures?,System Information,e.g. Yes it is used in power grid monitoring,,
5,Is the AI system intended to be used as safety component in management and operation of (a) digital critical infrastructure (b) road traffic or (c) supply of water gas heating or electricity?,System Information,e.g. Yes it is used for road traffic management,,
6,In which area (domain) your AI system will be used?,System Information,e.g. Healthcare,,
7,Is your system capable of adapting to new tasks autonomously?,System Information,e.g. No it requires manual fine-tuning for new tasks,,
8,Is the training validation and testing data sets subject to data governance and management practices appropriate for the intended purpose of the high-risk AI system? ,System Information,e.g. Yes data governance policies are in place,,
9,Is your system a General Purpose AI model?,System Information,e.g. Yes the system is a General Purpose AI model,,
10,Are the datasets labeled or annotated? If so who performed the labeling?,System Information,e.g. Yes data was labeled by domain experts,,
11,Were any data-cleaning steps applied before using the datasets?,System Information,Eliminate Duplicate or Irrelevant Data Correct Structural Errors Filter Outliers Address Missing Data Validate and Quality-Check,,
12,Does the dataset contain personally identifiable information ?,System Information,e.g. No all personal data was excluded,,
13,If personal data was used was it anonymized or pseudonymized?,System Information,e.g. Yes all personal data was anonymized,,
14,Were any external audits or reviews conducted on the dataset before use?,System Information,e.g. Yes an independent review was performed,,
15,Were any demographic groups underrepresented or overrepresented in the datasets?,System Information,e.g. Yes but adjustments were made to improve balance,,
16,Who will have to interpret system outputs in order to make decisions?,Generating stakeholders,e.g. Medical professionals will interpret results,,
17,Which demographic groups especially marginalized groups might be at risk of experiencing fairness harms?,Fairness Considerations,Marginalized groups may face exclusion bias or discrimination.,,
18,"Which harms might be experienced by demographic groups especially marginalized groups?
",Fairness Considerations,e.g. Discrimination unequal access to resources inaccurate predictions,,
19,Has an impact assessment been conducted to evaluate the potential discriminatory effects of the AI system?,Fairness Considerations,Yes or No,,
20,Were any data-cleaning steps applied before using the datasets?,System Information,Eliminate Duplicate or Irrelevant Data Correct Structural Errors Filter Outliers Address Missing Data Validate and Quality-Check,Data and Data Governance Requires that datasets used for training validation and testing be relevant representative free of errors and complete.,Annex IV Article 10 point 2
21,Are there any restrictions on the use of the datasets due to licensing or data protection laws?,System Information,Using personal data (e.g. names email addresses or health records) without explicit consent from individuals violates GDPR. For instance training an AI system on healthcare data without patient consent is illegal.,The EU AI Act prohibits certain uses of artificial intelligence (AI). These include AI systems that manipulate people's decisions or exploit their vulnerabilities systems that evaluate or classify people based on their social behavior or personal traits and systems that predict a person's risk of committing a crime.,rticle 5 – Prohibited AI Practices Article 6 Annex 5
22,How could someone misuse the system?,Potential impact of misuse,System misuse includes adversarial attacks unauthorized access or data manipulation.,,
23,Does the AI system function as a biometric identification system (uniquely identifying individuals) or as a biometric categorization system (grouping individuals based on shared characteristics)?,Biometric data,e.g. No it does not function as a biometric system,,
24,Does the AI system meet the definition of a Restricted Use?,Potentional Harm,e.g. No it is not classified as restricted use,,
25,What harms might this stakeholder experience if the system is not subject to appropriate human oversight and control?,Potentional Harm,e.g. Unfair decisions lack of accountability wrongful denial of services or inaccurate risk assessments.,,
26,What are the known limitations of the system?,System Information,data quality data availability lack of generalisation lack of transparency lack of explainability computational resources and scalability ethical implications Social Implications Dependency on Human Oversight Cybersecurity Risks?,,
27,Does the system provide explanations or justifications for its outputs?,System Information,e.g. No clear reasoning for decisions or Provides summary justifications,,
28,What were the results of the testing or validation ?,System Information,e.g. Accuracy Precision Recall F1-Score AUC-ROC,,
29,What methodologies were used for testing or validating the system?,System Information,cross-validation holdout method stress testing user testing and feedback simulation,,
30,Does the system require real-time decision-making or does it operate in a batch-processing manner?,System Information,Supports real-time decision-making and batch processing based on use case.,,
31,Does the system require data to function in real-world operation? If so what types of data are needed?,System Information,Requires structured and unstructured data like text numbers images sensors.,,
32,Was any personal data collected for training validation or testing?,System Information,Yes or No,,
33,What are the expected input types for the system?,System Information,Inputs include structured data unstructured text images and sensor data.,,
34,What are the expected output types of the system?,System Information,Outputs include scores classifications text visualizations and recommendations.,,
35,Does your system provides Back-Up Data ?,System Information,Yes backup mechanisms ensure data integrity and recovery.,,
36,Were any steps taken to assess the representativeness of the training data?,System Information,Training data assessed for demographic balance and diverse representation.,,
37,Was any bias identified in the training validation or testing datasets?,System Information,Bias identified particularly in underrepresented demographic groups.,,
38,If bias was detected what steps were taken to mitigate it?,System Information,Bias mitigated using data augmentation re-weighting and fairness-aware methods.,,
39,Who will be most directly involved in using or operating the system?,Generating stakeholders,System operators end users and domain experts manage and maintain it.,,
40,Who will be involved in the system design and development?,Generating stakeholders,AI researchers engineers data scientists and ethicists design and develop it.,,
41,Who are the stakeholders for each intended use?,Generating stakeholders,Users customers regulators developers and impacted communities are stakeholders.,,
42,Are the bias detection and mitigation strategies up-to-date?,Fairness Considerations,Yes or No,,
43,Are there mechanisms in place to address any identified discriminatory effects?,Fairness Considerations,Corrective actions include re-training refining data and fairness-aware algorithms.,,
44,What kind of human intervention can be found in your AI system ensuring a human-in-the-loop humanon-the-loop or human-in-command approach?,Human intervention,A human-in-the-loop or human-on-the-loop approach ensures oversight.,,
45,Is there any scenario where human intervention would not immediately be possible?,Human intervention,Immediate intervention isn’t possible in fully automated real-time applications.,,
46,How would misuse impact stakeholders?,Potential impact of misuse,Misuse risks misinformation unfair decisions security threats and reputational damage.,,
47,Do the consequences of misuse differ for any marginalized groups?,Potential impact of misuse,Marginalized groups face exclusion discrimination or heightened bias vulnerabilities.,,
48,Assess whether your system deduces sensitive attributes or if it categorises non-sensitive attributes,biometric data,,,
49,Could the AI system uphold or become a threat to human rights?,Potentional Harm,Responsible use upholds rights but misuse threatens privacy and fairness.,,
50,Could the AI system result in a risk of physical or psychological injury?,Potentional Harm,Errors in high-stakes applications risk physical or psychological harm.,,
51,What harms might this stakeholder experience if the AI system does not effectively solve the intended problem?,Potentional Harm,Stakeholders may face financial losses misinformation or reduced AI trust.,,
52,What harms might this stakeholder experience if they are unaware that they are interacting with an AI system when that system impersonates human interaction or generates or manipulates image audio or video content that could falsely appear to be authentic?,Potentional Harm,Deception and misinformation can harm trust and decision-making.,,
53,Is your AI system used in accordance with its intended purpose and under conditions of reasonably foreseeable misuse?,Risk Management,Safeguards exist but monitoring ensures proper usage and prevents misuse.,,