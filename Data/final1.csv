ID,Question,Section,Example,Information,Resources
1,What is the primary purpose of your AI system?,System Information,e.g. Chatbot for customer support ,,
2,Is the AI system intended to be used as a safety component of a product or is the AI system itself a product covered by the Union harmonisation legislation listed in Annex I AIA?,System Information,e.g. Yes it is a safety component in an autonomous vehicle.,,
3,Is the AI system itself a product being required to undergo a third-party conformity assessment with a view to the placing on the market or the putting into service of that product?,System Information,e.g. Yes it requires certification before deployment,,
4,Does the area (domain) of your AI system apply to critical infrastructures?,System Information,e.g. Yes it is used in power grid monitoring,,
5,Is the AI system intended to be used as safety component in management and operation of (a) digital critical infrastructure (b) road traffic or (c) supply of water gas heating or electricity?,System Information,e.g. Yes it is used for road traffic management,,
6,In which area (domain) your AI system will be used?,System Information,e.g. Healthcare,,
7,Is your system capable of adapting to new tasks autonomously?,System Information,e.g. No it requires manual fine-tuning for new tasks,,
8,Is the training validation and testing data sets subject to data governance and management practices appropriate for the intended purpose of the high-risk AI system? ,System Information,e.g. Yes data governance policies are in place,,
9,Is your system a General Purpose AI model?,System Information,e.g. Yes the system is a General Purpose AI model,,
10,Are the datasets labeled or annotated? If so who performed the labeling?,System Information,e.g. Yes data was labeled by domain experts,,
11,Were any data-cleaning steps applied before using the datasets?,System Information,Eliminate Duplicate or Irrelevant Data Correct Structural Errors Filter Outliers Address Missing Data Validate and Quality-Check,,
12,Does the dataset contain personally identifiable information ?,System Information,e.g. No all personal data was excluded,,
13,If personal data was used was it anonymized or pseudonymized?,System Information,e.g. Yes all personal data was anonymized,,
14,Were any external audits or reviews conducted on the dataset before use?,System Information,e.g. Yes an independent review was performed,,
15,Were any demographic groups underrepresented or overrepresented in the datasets?,System Information,e.g. Yes but adjustments were made to improve balance,,
16,Who will have to interpret system outputs in order to make decisions?,Generating stakeholders,e.g. Medical professionals will interpret results,,
17,Which demographic groups especially marginalized groups might be at risk of experiencing fairness harms?,Fairness Considerations,,,
18,"Which harms might be experienced by demographic groups, especially marginalized groups?
",Fairness Considerations,e.g. Discrimination unequal access to resources inaccurate predictions,,
19,Has an impact assessment been conducted to evaluate the potential discriminatory effects of the AI system?,Fairness Considerations,yes/no,,
20,Were any data-cleaning steps applied before using the datasets?,System Information,Eliminate Duplicate or Irrelevant Data Correct Structural Errors Filter Outliers Address Missing Data Validate and Quality-Check,Data and Data Governance Requires that datasets used for training validation and testing be relevant representative free of errors and complete.,Annex IV Article 10 point 2
21,Are there any restrictions on the use of the datasets due to licensing or data protection laws?,System Information,Using personal data (e.g. names email addresses or health records) without explicit consent from individuals violates GDPR. For instance training an AI system on healthcare data without patient consent is illegal.,The EU AI Act prohibits certain uses of artificial intelligence (AI). These include AI systems that manipulate people's decisions or exploit their vulnerabilities systems that evaluate or classify people based on their social behavior or personal traits and systems that predict a person's risk of committing a crime.,rticle 5 â€“ Prohibited AI Practices Article 6 Annex 5
22,How could someone misuse the system?,Potential impact of misuse,,,
23,Does the AI system function as a biometric identification system (uniquely identifying individuals) or as a biometric categorization system (grouping individuals based on shared characteristics)?,Biometric data,e.g. No it does not function as a biometric system,,
24,Does the AI system meet the definition of a Restricted Use?,Potentional Harm,e.g. No it is not classified as restricted use,,
25,"What harms might this stakeholder experience if the system is not subject to appropriate human oversight 
and control?",Potentional Harm,e.g. Unfair decisions lack of accountability wrongful denial of services or inaccurate risk assessments.,,
26,What are the known limitations of the system?,System Information,data quality data availability lack of generalisation lack of transparency lack of explainability computational resources and scalability ethical implications Social Implications Dependency on Human Oversight Cybersecurity Risks?,,
27,Does the system provide explanations or justifications for its outputs?,System Information,e.g. No clear reasoning for decisions or Provides summary justifications,,
28,What were the results of the testing or validation ?,System Information,"e.g. Accuracy, Precision, Recall, F1-Score, AUC-ROC",,
29,What methodologies were used for testing or validating the system?,System Information,cross-validation holdout method stress testing user testing and feedback simulation,,
30,Does the system require real-time decision-making or does it operate in a batch-processing manner?,System Information,,,
31,Does the system require data to function in real-world operation? If so what types of data are needed?,System Information,,,
32,Was any personal data collected for training validation or testing?,System Information,yes/no,,
33,What are the expected input types for the system?,System Information,,,
34,What are the expected output types of the system?,System Information,,,
35,Does your system provides Back-Up Data ?,System Information,,,
36,Were any steps taken to assess the representativeness of the training data?,System Information,,,
37,Was any bias identified in the training validation or testing datasets?,System Information,,,
38,If bias was detected what steps were taken to mitigate it?,System Information,,,
39,Who will be most directly involved in using or operating the system?,Generating stakeholders,,,
40,Who will be involved in the system design and development?,Generating stakeholders,,,
41,Who are the stakeholders for each intended use?,Generating stakeholders,,,
42,Are the bias detection and mitigation strategies up-to-date?,Fairness Considerations,yes/no,,
43,Are there mechanisms in place to address any identified discriminatory effects?,Fairness Considerations,,,
44,What kind of human intervention can be found in your AI system ensuring a human-in-the-loop humanon-the-loop or human-in-command approach?,Human intervention,,,
45,Is there any scenario where human intervention would not immediately be possible?,Human intervention,,,
46,How would misuse impact stakeholders?,Potential impact of misuse,,,
47,Do the consequences of misuse differ for any marginalized groups?,Potential impact of misuse,,,
48,Assess whether your system deduces sensitive attributes (e.g. race political opinions sexual orientation) or if it categorises non-sensitive attributes (e.g. hair colour eye colour).,biometric data,,,
49,Could the AI system uphold or become a threat to human rights?,Potentional Harm,,,
50,Could the AI system result in a risk of physical or psychological injury?,Potentional Harm,,,
51,"What harms might this stakeholder experience if the AI system does not effectively solve the intended 
problem?",Potentional Harm,,,
52,"What harms might this stakeholder experience if they are unaware that they are interacting with an AI system 
when that system impersonates human interaction or generates or manipulates image audio or video 
content that could falsely appear to be authentic?",Potentional Harm,,,
53,Is your AI system used in accordance with its intended purpose and under conditions of reasonably foreseeable misuse?,Risk Management,,,